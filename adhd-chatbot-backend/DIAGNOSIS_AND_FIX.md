# DiagnÃ³stico y SoluciÃ³n: Calidad del LLM Chatbot TDAH

## ğŸ“Š Root Cause Analysis

### Problema Reportado
El chatbot estÃ¡ generando respuestas de baja calidad ("diciendo cualquier cosa"), no coherentes, y posiblemente no respetando el idioma espaÃ±ol.

### Causas Identificadas

#### ğŸ”´ **CRÃTICO: Prompt Overload (Causa Principal)**

**Evidencia:**
- Prompt original: **9,136 caracteres** (~2,284 tokens)
- Modelo: Llama-3.2-1B-Instruct (modelo pequeÃ±o, 1 billÃ³n de parÃ¡metros)
- LÃ­mite de respuesta: 400 tokens

**Problema:**
Modelos pequeÃ±os (1B parÃ¡metros) se saturan con contextos largos. Cuando el prompt consume >2,000 tokens, el modelo:
- Pierde foco en instrucciones clave
- No puede priorizar quÃ© informaciÃ³n es relevante
- Genera respuestas incoherentes al intentar "procesar todo"
- Las instrucciones importantes (como "responde en espaÃ±ol") se pierden en el ruido

**Impacto:** **ALTO** - Esta es la causa principal de la mala calidad

---

#### ğŸŸ¡ **IMPORTANTE: Knowledge Base No Optimizada**

**Evidencia:**
```javascript
// promptBuilder.js (original)
const adhdKnowledge = getADHDKnowledge(); // ~6,000 caracteres
// Se inyecta TODO el conocimiento en CADA consulta
```

**Problema:**
- Se incluye conocimiento completo de TDAH + Scattered Minds en cada request
- 90% de ese conocimiento no es relevante para cada consulta especÃ­fica
- Desperdicia tokens del contexto con informaciÃ³n no utilizada

**Impacto:** **MEDIO** - Contribuye a saturaciÃ³n del modelo

---

#### ğŸŸ¡ **Estructura de Prompt No Optimizada**

**Evidencia:**
```javascript
// Original: Todo en un solo mensaje "user"
messages: [{ role: 'user', content: HUGE_PROMPT }]
```

**Problema:**
- No se separa system message de user message
- Instrucciones mezcladas con conocimiento y consulta
- Dificulta al modelo distinguir quÃ© es instrucciÃ³n vs contenido

**Impacto:** **MEDIO** - Afecta seguimiento de instrucciones

---

#### ğŸŸ¢ **libro1.md No Integrado (No es problema)**

**Evidencia:**
- Archivo libro1.md: 567,544 caracteres (92,005 palabras)
- Contiene libro completo "Scattered Minds"

**AnÃ¡lisis:**
- âœ… Ya tenemos conceptos clave extraÃ­dos en `scattered-minds-concepts.js`
- âœ… libro1.md sirve como referencia, NO debe inyectarse en prompts
- âœ… No es un problema, es material de consulta

**AcciÃ³n:** Ninguna necesaria - archivo correctamente excluido

---

#### ğŸŸ¢ **ParÃ¡metros del Modelo**

**ConfiguraciÃ³n actual:**
```javascript
max_tokens: 400
temperature: 0.75
top_p: 0.92
frequency_penalty: 0.2
presence_penalty: 0.2
```

**AnÃ¡lisis:**
- ParÃ¡metros razonables para el modelo
- Ajuste menor recomendado (ver soluciÃ³n)

**Impacto:** **BAJO** - OptimizaciÃ³n secundaria

---

## âœ… Soluciones Implementadas

### 1. **Prompt Optimizado (76-80% reducciÃ³n)**

#### Archivo: `knowledge/adhd-knowledge-base-optimized.js`

**Estrategia:**
- âœ… Knowledge base compacto (core concepts ~1,200 chars)
- âœ… InyecciÃ³n selectiva por patrÃ³n detectado
- âœ… Detector de patrones (parÃ¡lisis ejecutiva, foco, procrastinaciÃ³n, etc.)

**Resultados:**
```
Prompt original:  9,136 chars (~2,284 tokens)
Prompt optimizado: 1,801-2,174 chars (~450-544 tokens)
ReducciÃ³n: 76-80%
```

**Ejemplo: DetecciÃ³n de patrÃ³n "ParÃ¡lisis Ejecutiva"**
```javascript
// Mensaje: "Me siento abrumado y no sÃ© por dÃ³nde empezar"
detectPattern(message) // â†’ 'paralisis_ejecutiva'

// Se inyecta SOLO conocimiento relevante:
// - Core TDAH (~1,200 chars)
// - ParÃ¡lisis ejecutiva especÃ­fica (~500 chars)
// Total: ~1,700 chars vs 9,136 original
```

---

### 2. **SeparaciÃ³n System/User Messages**

#### Archivo: `services/llmService-optimized.js`

**Antes:**
```javascript
messages: [
  { role: 'user', content: "Todo mezclado: instrucciones + conocimiento + consulta" }
]
```

**DespuÃ©s:**
```javascript
messages: [
  {
    role: 'system',
    content: "Eres asistente TDAH. [Conocimiento + Instrucciones]"
  },
  {
    role: 'user',
    content: "CONTEXTO: [contexto]. CONSULTA: [pregunta]"
  }
]
```

**Beneficio:**
- Modelos pequeÃ±os siguen mejor instrucciones cuando estÃ¡n en system message
- SeparaciÃ³n clara de responsabilidades
- EspaÃ±ol forzado desde el principio: "Eres asistente TDAH. SIEMPRE respondes en ESPAÃ‘OL"

---

### 3. **ParÃ¡metros Ajustados**

```javascript
// Optimized
max_tokens: 350,        // Reducido de 400 (permite completar mejor)
temperature: 0.7,       // Reducido de 0.75 (mÃ¡s enfocado)
top_p: 0.9,             // Reducido de 0.92 (mejor coherencia)
frequency_penalty: 0.3, // Aumentado de 0.2 (reduce repeticiÃ³n)
```

**RazÃ³n:**
- Temperatura mÃ¡s baja = respuestas mÃ¡s determinÃ­sticas
- top_p mÃ¡s bajo = vocabulario mÃ¡s enfocado
- frequency_penalty mayor = menos repeticiÃ³n de frases

---

### 4. **Prompt Structure Simplificado**

**Antes:** 12 directrices complejas con sub-puntos

**DespuÃ©s:** 7 instrucciones directas y claras

```
INSTRUCCIONES OBLIGATORIAS:
1. Responde en ESPAÃ‘OL (nunca inglÃ©s)
2. ExtensiÃ³n: 120-180 palabras
3. Tono: empÃ¡tico pero profesional
4. Estructura OBLIGATORIA: validaciÃ³n + explicaciÃ³n + estrategias + herramienta
5. Usa emojis estratÃ©gicos: ğŸ“Œ ğŸ¯ ğŸ’¡ â±ï¸
6. Lenguaje de coaching: "podrÃ­as probar...", "funciona bien..."
7. NUNCA diagnostiques ni reemplaces terapia
```

**Beneficio:** Modelos pequeÃ±os procesan mejor instrucciones simples y directas

---

## ğŸ”„ CÃ³mo Implementar las Soluciones

### OpciÃ³n 1: Reemplazo Completo (Recomendado)

#### Paso 1: Backup de archivos originales
```bash
cd /home/nicolas/Trabajo\ Final\ de\ Grado/adhd-chatbot-backend

# Backup
cp services/llmService.js services/llmService.original.js
cp services/promptBuilder.js services/promptBuilder.original.js
cp knowledge/adhd-knowledge-base.js knowledge/adhd-knowledge-base.original.js
```

#### Paso 2: Reemplazar con versiones optimizadas
```bash
# Reemplazar LLM service
mv services/llmService-optimized.js services/llmService.js

# Reemplazar prompt builder
mv services/promptBuilder-optimized.js services/promptBuilder.js

# Agregar knowledge base optimizada (mantener original como backup)
# No reemplazar, solo agregar y cambiar imports
```

#### Paso 3: Actualizar imports en rutas
```javascript
// routes/chat.js
const { generateOptimizedResponse } = require('../services/llmService');

// O simplemente: generateResponse ahora apunta a optimized
const { generateResponse } = require('../services/llmService');
```

#### Paso 4: Reiniciar servidor
```bash
npm start
```

---

### OpciÃ³n 2: Prueba A/B (MÃ¡s Seguro)

Mantener ambas versiones y probar:

```javascript
// routes/chat.js
const { generateResponse: generateOld } = require('../services/llmService.original');
const { generateOptimizedResponse: generateNew } = require('../services/llmService');

// Usar optimizada por defecto
const response = await generateNew(message, context);

// Alternar con variable de entorno
const useOptimized = process.env.USE_OPTIMIZED_LLM !== 'false';
const generate = useOptimized ? generateNew : generateOld;
```

---

## ğŸ“ˆ MÃ©tricas de Mejora Esperadas

### Calidad de Respuestas

| MÃ©trica | Antes | DespuÃ©s (esperado) |
|---------|-------|---------------------|
| Coherencia | Baja (respuestas aleatorias) | Alta (estructura clara) |
| Adherencia a espaÃ±ol | Inconsistente | 95%+ (forzado en system) |
| Relevancia | Baja (informaciÃ³n genÃ©rica) | Alta (patrÃ³n-especÃ­fica) |
| Estructura | Ausente | Consistente (validaciÃ³n + estrategias) |
| MenciÃ³n herramientas app | <30% | >80% (obligatorio en prompt) |

### Performance

| MÃ©trica | Antes | DespuÃ©s |
|---------|-------|---------|
| Tokens de prompt | ~2,284 | ~450-544 |
| Tiempo de procesamiento | ~3-5 seg | ~2-3 seg (estimado) |
| Costo por request | Alto | -76% (reducciÃ³n tokens) |

---

## ğŸ§ª CÃ³mo Probar

### Test 1: Verificar estructura de prompt
```bash
cd /home/nicolas/Trabajo\ Final\ de\ Grado/adhd-chatbot-backend
node test-optimized-prompt.js
```

**Salida esperada:**
```
âœ… Dentro del target (<3000 chars)
ğŸ“‰ ReducciÃ³n: 76-80% vs prompt original
```

### Test 2: Probar con API real (requiere HF API key)
```bash
# Configurar API key
export HUGGING_FACE_API_KEY="tu_key_aqui"

# Ejecutar test
node test-llm-diagnosis.js
```

**Analiza:**
- âœ… Idioma detectado: EspaÃ±ol
- âœ… Contiene emojis: SÃ­
- âœ… Menciona herramientas app: SÃ­
- âœ… Tono empÃ¡tico: SÃ­
- âœ… Coherencia: Alta

### Test 3: Prueba en producciÃ³n (Render)

**Endpoint:** `https://adhd-chatbot-api.onrender.com/api/chat`

```bash
curl -X POST https://adhd-chatbot-api.onrender.com/api/chat \
  -H "Content-Type: application/json" \
  -d '{
    "message": "Me siento abrumado y no sÃ© por dÃ³nde empezar",
    "context": {
      "tasks": [{"isMandatory": true, "completed": false}],
      "pomodoroSessions": 0
    }
  }'
```

**Respuesta esperada (estructura):**
```json
{
  "response": "ğŸ“Œ Es completamente normal sentirse abrumado con TDAH... [validaciÃ³n]

  Esto sucede porque... [explicaciÃ³n neurobiolÃ³gica breve]

  1. Estrategia 1...
  2. Estrategia 2...

  ğŸ’¡ Puedes usar el Pomodoro de la app... [herramienta especÃ­fica]"
}
```

---

## ğŸš¨ Troubleshooting

### Problema: Respuestas siguen siendo en inglÃ©s

**Causa:** Modelo ignora instrucciÃ³n de idioma

**SoluciÃ³n:**
1. Verificar que `system` message incluya: "SIEMPRE respondes en ESPAÃ‘OL"
2. Aumentar Ã©nfasis en user prompt: "Responde en espaÃ±ol siguiendo la estructura..."
3. Agregar a stop tokens: palabras en inglÃ©s comunes

```javascript
stop: ['\n\n\n', '###', 'Usuario:', 'User:', 'ADHD symptoms', 'You might', 'Here are']
```

---

### Problema: Respuestas muy cortas (<50 palabras)

**Causa:** Modelo termina prematuramente

**SoluciÃ³n:**
1. Aumentar `min_tokens` (si API lo soporta)
2. Modificar prompt: "Responde EXACTAMENTE 120-180 palabras (no menos)"
3. Ajustar `temperature` a 0.75 (mÃ¡s variedad)

---

### Problema: Sigue sin mencionar herramientas de la app

**Causa:** InstrucciÃ³n no enfatizada suficiente

**SoluciÃ³n:**
```javascript
// En buildUserPrompt()
return `${context}

CONSULTA: "${message}"

OBLIGATORIO: Tu respuesta DEBE mencionar al menos UNA de estas herramientas:
- Temporizador Pomodoro (25 minutos)
- Modo ConcentraciÃ³n
- Ambientes Sonoros (ruido rosa/marrÃ³n)

Responde en espaÃ±ol, 120-180 palabras, siguiendo estructura obligatoria.`;
```

---

### Problema: API key no configurada

**Error:** `WARNING: HUGGING_FACE_API_KEY not properly configured`

**SoluciÃ³n:**

1. Obtener API key: https://huggingface.co/settings/tokens
2. Configurar localmente:
```bash
echo 'HUGGING_FACE_API_KEY=hf_tu_key_real_aqui' >> .env
```

3. Configurar en Render (producciÃ³n):
   - Dashboard â†’ Environment Variables
   - Agregar: `HUGGING_FACE_API_KEY = hf_...`
   - Redeploy

---

## ğŸ“ Archivos Creados/Modificados

### Nuevos Archivos (Optimizados)
```
adhd-chatbot-backend/
â”œâ”€â”€ knowledge/
â”‚   â””â”€â”€ adhd-knowledge-base-optimized.js  [NUEVO] â­
â”œâ”€â”€ services/
â”‚   â”œâ”€â”€ llmService-optimized.js           [NUEVO] â­
â”‚   â””â”€â”€ promptBuilder-optimized.js        [NUEVO] â­
â””â”€â”€ tests/
    â”œâ”€â”€ test-optimized-prompt.js          [NUEVO]
    â”œâ”€â”€ test-llm-diagnosis.js             [NUEVO]
    â”œâ”€â”€ test-prompt-structure.js          [NUEVO]
    â””â”€â”€ extract-libro1-concepts.js        [NUEVO]
```

### Archivos Originales (Mantener como backup)
```
adhd-chatbot-backend/
â”œâ”€â”€ knowledge/
â”‚   â”œâ”€â”€ adhd-knowledge-base.js            [ORIGINAL]
â”‚   â”œâ”€â”€ scattered-minds-concepts.js       [MANTENER - estÃ¡ bien]
â”‚   â””â”€â”€ libro1.md                         [MANTENER - referencia]
â”œâ”€â”€ services/
â”‚   â”œâ”€â”€ llmService.js                     [ORIGINAL]
â”‚   â””â”€â”€ promptBuilder.js                  [ORIGINAL]
```

---

## ğŸ¯ ConclusiÃ³n: Esperado vs Realidad

### Â¿Por quÃ© estaba fallando?

**AnalogÃ­a:** Es como darle a un estudiante:
- ğŸ“š Un libro de texto completo (9,136 chars)
- ğŸ“ Instrucciones complejas con 12 puntos
- â±ï¸ Y pedirle que responda en 2 minutos

**Resultado:** Respuestas apresuradas, incompletas, aleatorias

### Â¿QuÃ© hace la soluciÃ³n optimizada?

**AnalogÃ­a mejorada:** Ahora le damos:
- ğŸ“„ Solo la pÃ¡gina relevante (~2,000 chars)
- âœ… 7 instrucciones claras y directas
- â±ï¸ Tiempo suficiente para procesar

**Resultado esperado:** Respuestas coherentes, estructuradas, en espaÃ±ol

---

## ğŸš€ PrÃ³ximos Pasos Recomendados

1. **Implementar versiÃ³n optimizada** (OpciÃ³n 1 o 2)
2. **Probar con 10-15 consultas reales** de diferentes patrones
3. **Analizar respuestas** con criterios:
   - âœ… Idioma espaÃ±ol
   - âœ… Estructura clara (validaciÃ³n + explicaciÃ³n + estrategias + herramienta)
   - âœ… Emojis estratÃ©gicos
   - âœ… Coherencia y relevancia
4. **Ajustar parÃ¡metros** si necesario (temperature, top_p)
5. **Documentar ejemplos** de buenas respuestas para validaciÃ³n continua

---

## ğŸ“ Soporte

Si despuÃ©s de implementar las optimizaciones persisten problemas:

1. Verificar logs del servidor: `npm start` (buscar errores)
2. Revisar respuesta cruda de API (antes de cleanResponse)
3. Probar con modelo alternativo si disponible (ej: Llama-3.2-3B si hay budget)
4. Considerar MCP (Model Context Protocol) con cuenta HuggingFace del usuario

---

**Autor:** Claude Code (Diagnosis & Optimization)
**Fecha:** 15 de noviembre de 2025
**VersiÃ³n:** 1.0
