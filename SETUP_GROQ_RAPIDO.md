# ğŸš€ Setup Groq API - SoluciÃ³n RÃ¡pida para Recursos Limitados

## ğŸ¯ Â¿Por quÃ© Groq?

Tu servidor tiene recursos limitados:
- **RAM:** 5.8GB (insuficiente para modelos grandes)
- **CPU:** 1 vCPU (muy lento para inferencia local)

**Groq es PERFECTO para tu caso:**
- âœ… **GRATIS** (6000 tokens/min)
- âœ… **ULTRA RÃPIDO** (<1 segundo vs 10-30 seg local)
- âœ… **Modelo potente** (Llama 3.1 8B vs tu actual 1B)
- âœ… **Sin usar tus recursos** (0GB RAM)
- âœ… **Setup en 5 minutos**

---

## ğŸ“‹ Paso a Paso (5 minutos)

### 1. ObtÃ©n tu API Key de Groq (GRATIS)

1. Ve a: https://console.groq.com/keys
2. Crea una cuenta (gratis, sin tarjeta de crÃ©dito)
3. Click en "Create API Key"
4. Copia la clave (empieza con `gsk_...`)

### 2. Instala el SDK de Groq

```bash
cd adhd-chatbot-backend
npm install groq-sdk
```

### 3. Agrega tu API Key al `.env`

```bash
# Edita .env
nano .env

# Agrega esta lÃ­nea:
GROQ_API_KEY=gsk_TU_CLAVE_AQUI

# Guarda y cierra (Ctrl+X, Y, Enter)
```

### 4. Actualiza el servicio LLM

**OpciÃ³n A: Reemplazar completamente (Recomendado)**

```bash
# Backup del archivo actual
cp services/llmService.js services/llmService-huggingface-backup.js

# Reemplazar con Groq
cp services/llmService-groq.js services/llmService.js
```

**OpciÃ³n B: Dual con fallback (MÃ¡s robusto)**

Edita `routes/chat.js`:

```javascript
// Al inicio del archivo (despuÃ©s de otros requires)
const llmServiceGroq = require('../services/llmService-groq');
const llmServiceHF = require('../services/llmService'); // Fallback

// En la funciÃ³n que genera respuestas (alrededor de lÃ­nea 77)
try {
  // Intenta Groq primero
  const result = await llmServiceGroq.generateResponse(prompt);

  return res.json({
    response: result.response,
    model: result.model,
    tokensUsed: result.tokensUsed,
    responseTime: result.responseTime,
    provider: 'groq'
  });

} catch (error) {
  console.error('Groq failed, falling back to HuggingFace:', error.message);

  // Fallback a HuggingFace
  const hfResult = await llmServiceHF.generateResponse(prompt);

  return res.json({
    response: hfResult,
    model: 'huggingface-fallback',
    provider: 'huggingface'
  });
}
```

### 5. Reinicia tu servidor

```bash
# Si usas PM2
pm2 restart all

# Si usas npm/node directamente
npm start

# Verifica que estÃ© corriendo
curl http://localhost:3000/api/chat \
  -H "Content-Type: application/json" \
  -d '{"message": "Hola, Â¿quÃ© es el TDAH?"}'
```

---

## âœ… VerificaciÃ³n

### Test 1: Health Check

```bash
# Prueba la API
curl http://localhost:3000/api/chat \
  -H "Content-Type: application/json" \
  -d '{
    "message": "Explica el TDAH en 50 palabras",
    "context": {}
  }'
```

DeberÃ­as ver respuesta en **<2 segundos** (vs 10-30 seg con modelo local).

### Test 2: LÃ­mites de Tokens

```bash
# Prueba con mensaje largo
curl http://localhost:3000/api/chat \
  -H "Content-Type: application/json" \
  -d '{
    "message": "Dame una guÃ­a completa y detallada sobre cÃ³mo manejar el TDAH en adultos, incluyendo estrategias de organizaciÃ³n, concentraciÃ³n, y manejo de tareas",
    "context": {}
  }'
```

Ahora obtendrÃ¡s **respuestas mÃ¡s largas** (1500 tokens vs 450).

---

## ğŸ“Š ComparaciÃ³n Antes/DespuÃ©s

| Aspecto | Antes (HF Router) | DespuÃ©s (Groq) | Mejora |
|---------|-------------------|----------------|--------|
| **Modelo** | Llama 1B | Llama 3.1 8B | 8x mÃ¡s potente |
| **Tiempo respuesta** | 2-5 seg | <1 seg | 5x mÃ¡s rÃ¡pido |
| **Max tokens salida** | 450 | 1500+ | 3.3x mÃ¡s |
| **Calidad** | BÃ¡sica | Excelente | â†‘â†‘â†‘ |
| **Uso RAM servidor** | 0 | 0 | Igual |
| **Costo** | $0 | $0 | Igual |
| **Rate limit** | 20 req/min | 6000 tokens/min* | 300x mÃ¡s |

*Suficiente para ~60-100 respuestas/min

---

## ğŸ”§ ConfiguraciÃ³n Avanzada

### Aumentar lÃ­mites de tokens

En `llmService-groq.js`, lÃ­nea 34:

```javascript
// Aumenta segÃºn necesites
maxTokens = 2000,  // Era 1500
```

### Cambiar modelo

Groq ofrece varios modelos gratuitos:

```javascript
// llmService-groq.js, lÃ­nea 37
model = 'llama-3.1-8b-instant'        // RÃ¡pido (recomendado)
model = 'llama-3.1-70b-versatile'     // MÃ¡s potente pero mÃ¡s lento
model = 'mixtral-8x7b-32768'          // Contexto gigante (32K)
```

### Ajustar temperatura

```javascript
// LÃ­nea 35-36
temperature = 0.72,  // 0 = determinista, 1 = creativo
topP = 0.92,         // Top-p sampling
```

---

## ğŸš¨ Troubleshooting

### Error: "API key de Groq invÃ¡lida"

```bash
# Verifica que la clave estÃ© en .env
cat .env | grep GROQ_API_KEY

# Debe mostrar:
# GROQ_API_KEY=gsk_...

# Si no estÃ¡, agrÃ©gala:
echo "GROQ_API_KEY=gsk_TU_CLAVE" >> .env

# Reinicia servidor
pm2 restart all
```

### Error: "LÃ­mite de rate excedido"

Tier gratuito: 6000 tokens/min

**Soluciones:**
1. Reduce `maxTokens` a 1000
2. Implementa rate limiting en tu backend
3. Actualiza a tier de pago (muy barato: $0.10/M tokens)

### Respuestas lentas

Si tarda >2 segundos, verifica:

```bash
# Test directo a Groq
curl https://api.groq.com/openai/v1/chat/completions \
  -H "Authorization: Bearer $GROQ_API_KEY" \
  -H "Content-Type: application/json" \
  -d '{
    "model": "llama-3.1-8b-instant",
    "messages": [{"role": "user", "content": "test"}],
    "max_tokens": 10
  }'

# Debe responder en <1 segundo
```

### Groq no disponible (fallback a HF)

Si implementaste dual mode y ves:

```
Groq failed, falling back to HuggingFace
```

Es normal si:
- No hay internet
- Groq tiene downtime (raro)
- Se excediÃ³ rate limit

Tu app seguirÃ¡ funcionando con HuggingFace.

---

## ğŸ’° Costos y LÃ­mites

### Tier Gratuito (Forever Free)

| LÃ­mite | Valor |
|--------|-------|
| **Requests/min** | 30 |
| **Requests/dÃ­a** | 14,400 |
| **Tokens/min** | 6,000 |
| **Tokens/dÃ­a** | Ilimitado* |

*Mientras respetes lÃ­mites por minuto

**Para tu app:** Asumiendo 500 tokens/respuesta:
- **12 respuestas/minuto** = 720/hora = **17,280/dÃ­a**
- MÃ¡s que suficiente para una app personal

### Tier de Pago (opcional)

Si creces mucho:
- **$0.10 por millÃ³n de tokens** (input)
- **$0.10 por millÃ³n de tokens** (output)

Ejemplo: 10,000 respuestas/mes @ 500 tokens = $0.50/mes

Mucho mÃ¡s barato que hostear servidor propio.

---

## ğŸ“ Recursos Adicionales

### DocumentaciÃ³n Groq
- **Docs**: https://console.groq.com/docs
- **API Reference**: https://console.groq.com/docs/api-reference
- **Modelos disponibles**: https://console.groq.com/docs/models

### Monitoreo
- **Dashboard**: https://console.groq.com/dashboard
- Ve uso en tiempo real
- EstadÃ­sticas de requests

### Rate Limits
- **LÃ­mites actuales**: https://console.groq.com/settings/limits
- **Upgrade**: https://console.groq.com/settings/billing

---

## ğŸ†š Alternativas a Groq

Si por alguna razÃ³n no quieres usar Groq:

### 1. **Together.ai** (similar a Groq)
- $5 crÃ©dito gratis
- Llama 3.1 8B: $0.18/M tokens
- MÃ¡s lento que Groq

### 2. **OpenRouter** (agregador)
- Acceso a mÃºltiples modelos
- Free tier con modelos pequeÃ±os
- Llama 3.1 8B: ~$0.20/M tokens

### 3. **Perplexity API** (experimental)
- Free tier limitado
- Modelos propios + Llama

### 4. **Hostear localmente** (requiere upgrade)

Si upgradeas tu Oracle Cloud a instancia Ampere A1 (24GB RAM gratis):

```bash
# PodrÃ­as correr Phi-3-Mini localmente
ollama pull phi3:mini
# Respuestas en 5-10 seg con 4 CPUs
```

---

## âœ¨ Siguiente Paso: RAG (Opcional)

Una vez que Groq funcione, puedes agregar **RAG con libros especializados**:

1. Instala ChromaDB localmente (en tu laptop, no servidor)
2. Procesa libros de TDAH
3. Crea servicio que:
   - Busca contexto en ChromaDB
   - Llama a Groq con contexto enriquecido
4. Despliega servicio RAG en servidor separado

**Ventaja:** Groq + RAG = Respuestas expertas basadas en libros cientÃ­ficos

---

## ğŸ“ Soporte

**Archivo creado:** `adhd-chatbot-backend/services/llmService-groq.js`

**DocumentaciÃ³n completa:**
- Este archivo: `SETUP_GROQ_RAPIDO.md`
- Opciones Oracle Cloud: `GUIA_DEPLOYMENT_ORACLE.md`
- RAG: `rag-setup/README.md`

**Â¿Problemas?**
1. Revisa logs: `pm2 logs` o `npm start`
2. Verifica .env: `cat .env | grep GROQ`
3. Test API key: curl directo a Groq (ver troubleshooting)

---

## ğŸ‰ Â¡Listo!

Con Groq implementado tendrÃ¡s:
- âœ… Modelo 8x mÃ¡s potente
- âœ… Respuestas 5x mÃ¡s rÃ¡pidas
- âœ… 3x mÃ¡s tokens de salida
- âœ… Sin usar recursos de tu servidor
- âœ… **Gratis**

**Tiempo de setup: 5 minutos**

**PrÃ³ximos pasos:**
1. ObtÃ©n API key de Groq
2. `npm install groq-sdk`
3. Agrega GROQ_API_KEY a .env
4. Actualiza llmService.js
5. Reinicia servidor
6. Â¡Prueba!
